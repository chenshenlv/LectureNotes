{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurons and Neural Networks in the Human Brain\n",
    "\n",
    "* The brain is a complex, nonlinear information-processing system made up of a huge number of interconnected of *neurons*\n",
    "* The neuron in the human brain is five to six orders of magnitude slower than silicon logic gates ( $10^{-3}$s/millisecond vs. $10^{-9}$s/nanosecond ) \n",
    "* See http://biomedicalengineering.yolasite.com/neurons.php for an illustration of neurons in the brain. \n",
    "\n",
    "* *Synapse:* units that control interaction between neurons. A *Chemical synapse* is the most common kind of synapse.  It converts presynaptic electrical signal into a chemical signal and then back.  Traditional descriptions of assume that a synapse can either impose *excitation* or *inhibition* on the receptive neuron.\n",
    "* See: http://science.education.nih.gov/supplements/nih2/addiction/guide/lesson2-1.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief ANN History\n",
    "\n",
    "* 1943 McCulloch and Pitts.  McCulloch was a psychatrist and neuroanatomist.  Pitts was a mathematician.  They published a widley read article that introduced the idea of neural networks as computing machines. Their goal was to develop a model/understand how neurons in the brain might work.  They showed a range of arithmetic and logical functions their neuron could compute.  \n",
    "* 1949 Hebb wrote \"The Organization of Behavior\" which postulated (among many other things): \"When an axon of cell A is near enough to excite cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased\"  or \"when neurons fire together they wire together\"  This has come to be known as Hebbian learning. \n",
    "* 1954 Minsky wrote a \"neural network\" doctoral thesis at Princeton. \n",
    "* 1958 Rosenblatt introduced his work in the perceptron and he came up with the perceptron convergence theorem. First model for learning with a teacher (i.e., \"supervised learning\")\n",
    "* 1969 Minsky and Papert demonstrated the limits of the perceptron. They introduce multi-layer perceptrons but the published limits had the biggest influence - and interest dropped away. The AI winter began. \n",
    "* 1974 Werbos' Ph.D. thesis at Harvard developed back-propagation\n",
    "* 1986, the book \"Parallel Distributed Processing: Explorations in the Microstructures of Cognition\" was published and it covered back-propagation.  This made NN popular again. \n",
    "* Early 90's, SVMs overtook ANNs in popularity due to a number of challenges/downsides to ANNs in comparison to SVMs.  This included that SVMs were less likely to overtrain and easier to get good results on. Also, ANNs were very slow to train and had issues when they became \"deep\".\n",
    "* 2012 ImageNet challenge won by Hinton's team using a deep CNN (based on top 5 error rate, given an image, the model does not output the correct label within its top 5 predictions). They had an error rate of 15.4\\% (which was way better than 2nd place as 26.6\\%) This started the current DL/ANN resurgance. Now its HUGE.  \n",
    "* *What do you think will happen next?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenblatt's Perceptron\n",
    "\n",
    "* In an artificial neural network, we attempt to model the human neural network and neurons using programming constructs\n",
    "* A basic model for a neuron consists of the following: \n",
    "     * A set of *synapses* each of which is characterized by a *weight* (which includes a *bias*).\n",
    "     * An *adder*\n",
    "     * An *activiation function* (e.g. Rectified Linear Unit, Heaviside function, Piece-wise linear function, Sigmoid function)\n",
    "\n",
    "* We can write this mathematically as: \n",
    "\\begin{equation}\n",
    "y_k = \\phi\\left( \\sum_{j=1}^m w_{kj}x_j + b_k\\right)\n",
    "\\end{equation}\n",
    "* *What does this look like graphically?*\n",
    "\n",
    "# Perceptron Learning Algorithm\n",
    "* Suppose we have a ``neural network\" made of only one neuron - i.e., Rosenblatt's perceptron and we would like to train it to distinguish between two classes. \n",
    "* For Rosenblatt's perceptron to be effective, the classes must be linearly separable.  Mathematically, the classes must satisfy:\n",
    "\\begin{equation}\n",
    "\\exists \\mathbf{w} \\mid \\mathbf{w}^T\\mathbf{x} > 0 \\quad \\forall \\mathbf{x} \\in C_1;  \\quad \\mathbf{w}^T\\mathbf{x} \\le 0 \\quad \\forall \\mathbf{x} \\in C_2  \n",
    "\\end{equation}\n",
    "* The Perceptron Learning Algorithm will converge to the correct solution if the classes are linearly separable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import textwrap\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "def generateMVNRandData(Npts, mu, sigma):\n",
    "\tdata  = np.random.multivariate_normal(mu, sigma*np.eye(len(mu)), Npts)\n",
    "\treturn data\n",
    "\n",
    "def plotLine(weights, range):\n",
    "\tx = np.array(range)\n",
    "\ty = -(weights[0]/weights[1])-(weights[2]/weights[1])*x\n",
    "\tplt.plot(y,x)\n",
    "\tplt.pause(2)\n",
    "\n",
    "def perceptronLearningAlg(data,labels,eta,nIterations):\n",
    "\tnPts = data.shape[0]\n",
    "\tweights = np.random.rand(data.shape[1])\n",
    "\tprint('Initial weights:', weights)\n",
    "\n",
    "\terror = 1;\n",
    "\titer = 0;\n",
    "\twhile(error > 0 & iter < nIterations):\n",
    "\t\tprint('Iteration: ', iter,'; Error: ', error)\n",
    "\t\terror = 0\n",
    "\t\titer += 1\n",
    "\t\tfor i in range(nPts):\n",
    "\t\t\tactivation =  data[i,:]@weights\n",
    "\t\t\tactivation = (activation>0)\n",
    "\t\t\tif (activation-labels[i])!=0:\n",
    "\t\t\t\tweights-=eta*data[i,:]*(activation-labels[i])\n",
    "\t\t\t\terror += 1\n",
    "\t\t\t\tplotLine(weights, [-2,2])\n",
    "\n",
    "\tplotLine(weights, [-2,2])\n",
    "\tprint('Final Iteration: ', iter,'; Final Error: ', error)\n",
    "\treturn weights\n",
    "\n",
    "\n",
    "Npts  = 100\n",
    "mu1   = [2,2]\n",
    "mu2   = [0,0]\n",
    "var   = .1\n",
    "eta   = 10\n",
    "nIterations = 10;\n",
    "\n",
    "fig   = plt.figure()\n",
    "\n",
    "data1 = np.array(generateMVNRandData(Npts, mu1, .1))\n",
    "data1 = np.hstack((np.ones((Npts,1)),data1))\n",
    "data2 = np.array(generateMVNRandData(Npts, mu2, .1))\n",
    "data2 = np.hstack((np.ones((Npts,1)),data2))\n",
    "\n",
    "data  = np.vstack(( data1, data2))\n",
    "labels= np.hstack((np.ones(Npts), np.zeros(Npts)))\n",
    "\n",
    "plt.scatter(data[:,1],data[:,2], c=labels, linewidth=0)\n",
    "plt.pause(2)\n",
    "\n",
    "perceptronLearningAlg(data,labels,eta,nIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions to consider: \n",
    "* How does the learning behavior change with changes in $\\eta$? as $\\eta$ increases? as $\\eta$ decreases? \n",
    "* How would you generate overlapping classes using the provided code? Explain your answer. (Only change parameters.  You do not need to change code. )  \n",
    "* What happens to the learning behavior when you have overlapping classes? \n",
    "* The implementation provided uses \\{0,1\\} labels whereas the psuedo-code above uses \\{-1,1\\}.  How does the code need to change to account for this difference (i.e., suppose you want to use \\{-1,1\\} labels.  What would you need to change in the code?) Why?\n",
    "* In the provided code, there is not a separate line for learning the bias $b$as in the psuedo-code above.  How is it being estimated and represented it in the code (... the code IS still learning the bias value). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
